{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihS7ddRGnXKY",
        "outputId": "b6be3550-5dcf-48c2-f293-31bf2b267869"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import glob\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import string\n",
        "#porter = nltk.stem.PorterStemmer()\n",
        "import time\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KQaxcf0nXKa"
      },
      "source": [
        "# Useful function to creat our dataset\n",
        "\n",
        "def read(fileName):  # Function reading a txt file and giving as output the number of occurence of each word (Only word)\n",
        "    f = open(fileName,encoding=\"utf-8\")\n",
        "    m = f.read()\n",
        "    return m\n",
        "\n",
        "def preprocessing(corpus):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    training_data = []\n",
        "    sentences = corpus.split(\".\")\n",
        "    \n",
        "    sentences = sentences[5:] #put off five first sentence which is note in the book\n",
        "    nbSentences = len(sentences)\n",
        "    \n",
        "    #Take only a part of sentences otherwise computation time is too much long\n",
        "    bound = int(nbSentences/10) # -> 785 sentences otherwise we have more than 7000 sentences\n",
        "    sentences = sentences[:bound]\n",
        "    #sentences = sentences[:10] #smaller dataset\n",
        "    \n",
        "    for i in range(len(sentences)):\n",
        "        sentences[i] = sentences[i].strip()\n",
        "        sentence = sentences[i].split()\n",
        "        x = [word.strip(string.punctuation) for word in sentence if word not in stopwords.words('french') + stopwords.words('english')]\n",
        "        x = [word.lower() for word in x]\n",
        "        training_data.append(x)\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def prepare_data_for_training(sentences,window_size): #, w2v):\n",
        "    data = {}\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word not in data:\n",
        "                data[word] = 1\n",
        "            else:\n",
        "                data[word] += 1\n",
        "\n",
        "    V = len(data)\n",
        "\n",
        "    data = sorted(list(data.keys()))\n",
        "    vocab = {}\n",
        "    for i in range(len(data)):\n",
        "        vocab[data[i]] = i\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for i in range(len(sentence)):\n",
        "            center_word = [0 for x in range(V)]\n",
        "            center_word[vocab[sentence[i]]] = 1\n",
        "            context = [0 for x in range(V)]\n",
        "\n",
        "            for j in range(i - window_size, i + window_size):\n",
        "                if i != j and j >= 0 and j < len(sentence):\n",
        "                    context[vocab[sentence[j]]] += 1\n",
        "                    \n",
        "                    \n",
        "            X_train.append(center_word)\n",
        "            y_train.append(context)         \n",
        "\n",
        "    return X_train, y_train,V,data\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNT5cxgGnXKc"
      },
      "source": [
        "times = []\n",
        "losses = []\n",
        "likelihoods = []\n",
        "##Small parameter: 10sentences,alpha= 0.005,lamba = 0.001,embed = 1000,epoch = 70 -> convergence more epoch learning too slow\n",
        "##big parameter: 10sentences,alpha= 0.002 (if bigger, divergence),lamba = 0.00001,embed = 10,epoch = 30 -> convergence more epoch learning too slow\n",
        "# Model object\n",
        "class word2vec(object):\n",
        "    def __init__(self):\n",
        "        self.embedding_size = 10\n",
        "        self.alpha_coeff = 0.002\n",
        "        self.word_index = {}\n",
        "        self.window_size = 2\n",
        "        self.words = []\n",
        "        self.X_train = []\n",
        "        self.y_train = []\n",
        "        self.tag = []\n",
        "\n",
        "\n",
        "    def initialize(self, V, data):\n",
        "        self.V = V\n",
        "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.embedding_size))#Can have an impact\n",
        "        self.W_hidden = np.random.uniform(-0.8, 0.8, (self.embedding_size, self.V))\n",
        "\n",
        "        self.words = data\n",
        "        for i in range(len(data)):\n",
        "            self.word_index[data[i]] = i\n",
        "\n",
        "    def forward_compute(self, X):\n",
        "        self.h = np.dot(self.W.T, X).reshape(self.embedding_size, 1)\n",
        "        self.u = np.dot(self.W_hidden.T, self.h)\n",
        "        self.y = softmax(self.u)\n",
        "        return self.y\n",
        "\n",
        "    def gradient_descend(self, x, t):\n",
        "        e = self.y - np.asarray(t).reshape(self.V, 1)\n",
        "        dW_hidden = np.dot(self.h, e.T)\n",
        "        X = np.array(x).reshape(self.V, 1)\n",
        "        dLdW = np.dot(X, np.dot(self.W_hidden, e).T)\n",
        "        self.W_hidden = self.W_hidden - self.alpha_coeff * dW_hidden\n",
        "        self.W = self.W - self.alpha_coeff * dLdW\n",
        "\n",
        "    def train(self, epochs):\n",
        "\n",
        "        # ----------   SkipGram model ----------\n",
        "\n",
        "        coeff_lambda = 0.00001\n",
        "        # Computing likelihood take much more time so we decided to directly compute the loss function and analyse its evolution\n",
        "        # which is equivalent. We minimize the negative log likelihood which is the loss function.\n",
        "        # Because we can rewrite loss function with two sum to reduce operations.\n",
        "        \n",
        "        # for x in range(1, epochs):\n",
        "        #     start_time = time.time()\n",
        "        #     self.loss = 0\n",
        "        #     self.likelihood = 0\n",
        "            \n",
        "        #     for j in range(len(self.X_train)):\n",
        "        #         self.forward_compute(self.X_train[j])\n",
        "        #         self.gradient_descend(self.X_train[j], self.y_train[j])\n",
        "        #         C = 0\n",
        "        #         for m in range(self.V):\n",
        "        #             if (self.y_train[j][m]):\n",
        "                          \n",
        "        #                 self.loss += -1 * self.u[m][0] \n",
        "        #                 C += 1\n",
        "\n",
        "        #         self.loss += C * np.log(np.sum(np.exp(self.u)))\n",
        "                \n",
        "        #         #self.likelihood = coeff_lambda * self.likelihood  #negative sampling\n",
        "        #         #self.loss = -np.log(self.likelihood) #negative sampling\n",
        "            \n",
        "        #     # Computations values\n",
        "        #     self.loss = self.loss * coeff_lambda \n",
        "        #     self.likelihood = np.exp((-self.loss) )\n",
        "        #     print(\"epoch \", x, \" loss = \", self.loss, \" likelihood: \", self.likelihood)\n",
        "        #     self.alpha_coeff *= 1 / ((1 + self.alpha_coeff * x))\n",
        "        #     times.append((time.time() - start_time))\n",
        "        #     losses.append(self.loss)\n",
        "        #     likelihoods.append(self.likelihood)\n",
        "            \n",
        "        # ----------   SkipGram model ----------\n",
        "\n",
        "        # ----------  Negative Sampling Model --------\n",
        "\n",
        "        for x in range(1, epochs):\n",
        "            start_time = time.time()\n",
        "            self.loss = 0\n",
        "            self.likelihood = 0\n",
        "            \n",
        "            for j in range(len(self.X_train)):\n",
        "                self.forward_compute(self.X_train[j])\n",
        "                self.gradient_descend(self.X_train[j], self.y_train[j])\n",
        "\n",
        "                for m in range(self.V):\n",
        "                  if (self.y_train[j][m] == 1):\n",
        "                    self.likelihood += self.tag[j]*np.log(sigmoid(self.u[m][0])) + (1-self.tag[j])*np.log(1 - sigmoid(self.u[m][0]))\n",
        "                \n",
        "            # Computations values\n",
        "            self.likelihood = -1 * coeff_lambda * self.likelihood\n",
        "            self.loss = -np.log(self.likelihood)\n",
        "            print(\"epoch \", x, \" loss = \", self.loss, \" likelihood: \", self.likelihood)\n",
        "            self.alpha_coeff *= 1 / ((1 + self.alpha_coeff * x))\n",
        "            times.append((time.time() - start_time))\n",
        "            losses.append(self.loss)\n",
        "            likelihoods.append(self.likelihood)\n",
        "\n",
        "\n",
        "\n",
        "        # ----------  END Negative Sampling Model --------\n",
        "            \n",
        "\n",
        "    def predict(self, word, number_of_predictions):\n",
        "        if word in self.words:\n",
        "            index = self.word_index[word]\n",
        "            X = [0 for i in range(self.V)]\n",
        "            X[index] = 1\n",
        "            prediction = self.forward_compute(X)\n",
        "            output = {}\n",
        "            for i in range(self.V):\n",
        "                output[prediction[i][0]] = i\n",
        "\n",
        "            top_context_words = []\n",
        "            for k in sorted(output, reverse=True):\n",
        "                top_context_words.append(self.words[output[k]])\n",
        "                if (len(top_context_words) >= number_of_predictions):\n",
        "                    break\n",
        "\n",
        "            return top_context_words\n",
        "        else:\n",
        "            print(\"Word not found in dicitonary\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6VJpuHKnXKc"
      },
      "source": [
        "\n",
        "# Main\n",
        "corpus = read(\"VingMilleLieues.txt\")\n",
        "window_size = 2\n",
        "training_data = preprocessing(corpus)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDFvjB4HsfdF"
      },
      "source": [
        "def create_dataset(training_data,window_size,Negative_sampling,n):\n",
        "  X_train = []\n",
        "  y_train = []\n",
        "\n",
        "  Vocabulary = []\n",
        "\n",
        "  if Negative_sampling:\n",
        "    print(\"Negative sampling done here\")\n",
        "    print(\"Number of samplings :\", n)\n",
        "\n",
        "  for sentence in training_data:\n",
        "\n",
        "    for word in sentence:\n",
        "        if word not in Vocabulary:\n",
        "          Vocabulary.append(word)\n",
        "\n",
        "    for target in (sentence[window_size:len(sentence)-2]):\n",
        "        \n",
        "\n",
        "      index = sentence.index(target)\n",
        "      X_train.append((target,sentence[index-1]))\n",
        "      X_train.append((target,sentence[index-2]))\n",
        "      X_train.append((target,sentence[index+1]))\n",
        "      X_train.append((target,sentence[index+2]))\n",
        "      y_train.append(1)\n",
        "      y_train.append(1)\n",
        "      y_train.append(1)\n",
        "      y_train.append(1)\n",
        "\n",
        "      if Negative_sampling:\n",
        "        for i in range(n):\n",
        "          ind = random.randint(index,index+10)\n",
        "          ind = ind % len(sentence)\n",
        "          X_train.append((target,sentence[ind]))\n",
        "          y_train.append(0)\n",
        "\n",
        "         \n",
        "  return X_train,y_train,Vocabulary\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEsxoo6X2rxH"
      },
      "source": [
        "def hot_vector(data,Vocabulary):\n",
        "\n",
        "    X_coded = []\n",
        "    y_coded = []\n",
        "\n",
        "    for tup in X:\n",
        "      target = tup[0]\n",
        "      context = tup[1]\n",
        "\n",
        "      # print(\"target : \",target)\n",
        "      # print(\"context : \",context)\n",
        "\n",
        "      a = [0 for i in range(len(Vocabulary))]\n",
        "      b = [0 for i in range(len(Vocabulary))]\n",
        "\n",
        "      a[Vocabulary.index(target)] = 1\n",
        "      b[Vocabulary.index(context)] = 1\n",
        "\n",
        "      X_coded.append(a)\n",
        "      y_coded.append(b)\n",
        "\n",
        "    return X_coded,y_coded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjfGhlnusGu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673b195d-5b84-4856-d3bc-dcec65567c0d"
      },
      "source": [
        "X,tag,Vocabulary = create_dataset(training_data,2,True,2)\n",
        "print(X[0:12])\n",
        "print(tag[0:12])\n",
        "print(len(Vocabulary))\n",
        "print(Vocabulary[0:10])\n",
        "\n",
        "print(\"\")\n",
        "print(\"***************\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "X_coded , y_coded = hot_vector(X,Vocabulary)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative sampling done here\n",
            "Number of samplings : 2\n",
            "[('rumeurs', 'parler'), ('rumeurs', 'sans'), ('rumeurs', 'agitaient'), ('rumeurs', 'populations'), ('rumeurs', 'agitaient'), ('rumeurs', 'mer'), ('agitaient', 'rumeurs'), ('agitaient', 'parler'), ('agitaient', 'populations'), ('agitaient', 'ports'), ('agitaient', 'populations'), ('agitaient', 'public')]\n",
            "[1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0]\n",
            "3679\n",
            "['sans', 'parler', 'rumeurs', 'agitaient', 'populations', 'ports', 'surexcitaient', \"l'esprit\", 'public', \"l'intérieur\"]\n",
            "\n",
            "***************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQsKm1tcnXKd",
        "outputId": "aec280f5-5760-4e14-dfd7-c28508a203e9"
      },
      "source": [
        "w2v = word2vec()\n",
        "\n",
        "w2v.X_train = X_coded\n",
        "w2v.y_train = y_coded\n",
        "w2v.tag = tag\n",
        "w2v.initialize(len(Vocabulary), Vocabulary)\n",
        "\n",
        "epochs = 31\n",
        "w2v.train(epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  1  loss =  1.2049196312213961  likelihood:  0.29971608636110686\n",
            "epoch  2  loss =  1.2113547733780936  likelihood:  0.29779356322802586\n",
            "epoch  3  loss =  1.2143269027642416  likelihood:  0.29690979621327446\n",
            "epoch  4  loss =  1.2147231535159895  likelihood:  0.2967921687898402\n",
            "epoch  5  loss =  1.2115901185215836  likelihood:  0.29772348720550684\n",
            "epoch  6  loss =  1.2053463921583965  likelihood:  0.299588206532256\n",
            "epoch  7  loss =  1.2011801146894243  likelihood:  0.3008389778480506\n",
            "epoch  8  loss =  1.1985442466868346  likelihood:  0.3016329956870767\n",
            "epoch  9  loss =  1.195628427415752  likelihood:  0.3025137864780361\n",
            "epoch  10  loss =  1.1927259988832877  likelihood:  0.3033930865589202\n",
            "epoch  11  loss =  1.1899396949619274  likelihood:  0.3042396106948076\n",
            "epoch  12  loss =  1.1872698467854808  likelihood:  0.30505296955381606\n",
            "epoch  13  loss =  1.1846499791566831  likelihood:  0.30585321576567026\n",
            "epoch  14  loss =  1.182024516947262  likelihood:  0.30665727687945754\n",
            "epoch  15  loss =  1.1792977406654053  likelihood:  0.3074946037514233\n",
            "epoch  16  loss =  1.17635628575531  likelihood:  0.3084004168144937\n",
            "epoch  17  loss =  1.17314823296329  likelihood:  0.30939137029838615\n",
            "epoch  18  loss =  1.169690057742902  likelihood:  0.3104631520049061\n",
            "epoch  19  loss =  1.1660484838013612  likelihood:  0.31159578756546497\n",
            "epoch  20  loss =  1.1623237481915243  likelihood:  0.3127585636637257\n",
            "epoch  21  loss =  1.1586213395301141  likelihood:  0.3139186699473294\n",
            "epoch  22  loss =  1.1550204932551382  likelihood:  0.31505108041508384\n",
            "epoch  23  loss =  1.1515605098430755  likelihood:  0.31614303991879267\n",
            "epoch  24  loss =  1.148248819792069  likelihood:  0.31719174321046273\n",
            "epoch  25  loss =  1.1450767230768542  likelihood:  0.31819950360889515\n",
            "epoch  26  loss =  1.1420309831040654  likelihood:  0.31917013394983224\n",
            "epoch  27  loss =  1.139099309306152  likelihood:  0.32010720959751315\n",
            "epoch  28  loss =  1.1362720542171063  likelihood:  0.32101351490933766\n",
            "epoch  29  loss =  1.133542219646845  likelihood:  0.3218910258848226\n",
            "epoch  30  loss =  1.1309048680219986  likelihood:  0.3227410861661706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRAFwkaynXKd"
      },
      "source": [
        "#Plot graphs\n",
        "x = [i for i in range(epochs-1)]\n",
        "\n",
        "plt.plot(x,losses)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Loss Evolutions\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(x,times)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"times\")\n",
        "plt.title(\"Times Evolutions\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(x,likelihoods)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"likelihood\")\n",
        "plt.title(\"Likelihood Evolutions\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDSnf1FZnXKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8afd7a3f-6d41-4e70-da2c-cb3be1213430"
      },
      "source": [
        "#Result for 10 very frequent words\n",
        "\n",
        "def removeOne(liste):\n",
        "    for element in liste:\n",
        "        if len(element)<2 or element == '»' or element == '«':\n",
        "            liste.remove(element)\n",
        "    return liste\n",
        "    \n",
        "\n",
        "# print(\"mer: \",w2v.predict(\"mer\", 5))\n",
        "# print(\"land: \",w2v.predict(\"land\", 5))\n",
        "# print(\"capitaine: \",w2v.predict(\"capitaine\", 5))\n",
        "# print(\"monsieur: \",w2v.predict(\"monsieur\", 5))\n",
        "# print(\"conseil: \",w2v.predict(\"conseil\", 5))\n",
        "\n",
        "# print(\"être: \",w2v.predict(\"être\", 5))\n",
        "# print(\"dit: \",w2v.predict(\"dit\",5))\n",
        "# print(\"deux: \",w2v.predict(\"deux\", 5))\n",
        "# print(\"dont: \",w2v.predict(\"dont\", 5))\n",
        "# print(\"si: \",w2v.predict(\"si\", 5))\n",
        "\n",
        "\n",
        "print(\"mer: \",removeOne(w2v.predict(\"mer\", 8))[1:])\n",
        "print(\"capitaine: \",removeOne(w2v.predict(\"capitaine\", 5)))\n",
        "print(\"monsieur: \",removeOne(w2v.predict(\"monsieur\", 9)))\n",
        "print(\"conseil: \",removeOne(w2v.predict(\"conseil\", 8))[1:])\n",
        "\n",
        "print(\"être: \",removeOne(w2v.predict(\"être\", 5)))\n",
        "print(\"dit: \",removeOne(w2v.predict(\"dit\", 8))[1:])\n",
        "print(\"deux: \",removeOne(w2v.predict(\"deux\", 7)))\n",
        "print(\"dont: \",removeOne(w2v.predict(\"dont\", 8))[1:])\n",
        "print(\"si: \",removeOne(w2v.predict(\"si\", 8)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mer:  ['ned', 'plus', 'monsieur', 'trois', 'deux', 'commandant']\n",
            "capitaine:  ['ned', \"d'un\", 'plus', 'cent', 'si']\n",
            "monsieur:  ['monsieur', 'conseil', 'plus', 'commandant', 'répondit', 'si']\n",
            "conseil:  ['monsieur', 'conseil', 'ned', 'si', 'deux']\n",
            "être:  ['plus', 'répondit', 'pression', 'cent']\n",
            "dit:  ['ned', 'monsieur', 'deux', 'cent', 'plus']\n",
            "deux:  ['deux', 'mille', 'cent', 'trois', 'pieds', 'cette']\n",
            "dont:  ['ned', 'land', 'farragut', 'cette', 'plus']\n",
            "si:  ['si', 'cent', 'monsieur', '«', 'deux', 'bien']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhSxUgHgnXKe"
      },
      "source": [
        "## Negatif Sampling\n",
        "\n",
        "# Skip-gram Negative Sampling (SGNS) helps to speed up training time and improve quality\n",
        "# of resulting word vectors. This is done by training the network to only modify a small percentage \n",
        "# of the weights rather than all of them. Recall in our example above, we update the weights \n",
        "# for every other word and this can take a very long time if the vocab size is large. With SGNS, \n",
        "# we only need to update the weights for the target word and a small number (e.g. 5 to 20) of random ‘negative’ words."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}